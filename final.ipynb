{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_UNIQNAME = 'yuqin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import spacy\n",
    "import os\n",
    "from os import path\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import pathlib\n",
    "import json\n",
    "\n",
    "# Filter all warnings.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# loading up the language model: English\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 4])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use X_train, X_test, y_train, y_test for all of the following questions\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('training.csv',encoding='latin-1',names=['polarity','id','date','query','user','text'])\n",
    "df = df.iloc[780000:820001,:]\n",
    "df.polarity.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def text_clean(book_text):\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text_low = book_text.lower()\n",
    "    \n",
    "    # Remove punctuation and any other non-alphabet characters\n",
    "    text_low_no_num = re.sub(r'[0-9]', '', text_low)\n",
    "    \n",
    "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~''' # list of special characters you want to exclude\n",
    "    text_low_no_num_no_punc = \"\"\n",
    "    for char in text_low_no_num:\n",
    "        if char not in punctuations:\n",
    "            text_low_no_num_no_punc = text_low_no_num_no_punc + char\n",
    "            \n",
    "    clean_text = text_low_no_num_no_punc.replace(os.linesep, \"\")\n",
    "    \n",
    "    # stop words\n",
    "    text_nonstop = \"\"\n",
    "    words = clean_text.split()\n",
    "    for word in words:\n",
    "        if word not in STOP_WORDS:\n",
    "            text_nonstop = text_nonstop + \" \" + word \n",
    "    \n",
    "    return text_nonstop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.06 s, sys: 73.7 ms, total: 3.13 s\n",
      "Wall time: 3.13 s\n",
      "(40001, 65)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_df=0.8, max_features=200000,\n",
    "    min_df=0.2, stop_words='english',\n",
    "    use_idf=True, tokenizer=text_clean, ngram_range=(1,3)\n",
    ")\n",
    "\n",
    "%time tfidf_matrix = tfidf_vectorizer.fit_transform(document)\n",
    "\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "dist = 1 - cosine_similarity(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "reduced_data = PCA(n_components=5).fit_transform(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use seaborn's .clustermap() function to draw a hierarchically-clustered heatmap\n",
    "sns.clustermap(reduced_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "reduced_data = PCA(n_components=3).fit_transform(dist)\n",
    "# # Use seaborn's .clustermap() function to draw a hierarchically-clustered heatmap\n",
    "sns.clustermap(reduced_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Insert your code here\n",
    "\n",
    "# Rule of Thumb \n",
    "k = np.sqrt(30000/2)\n",
    "print(k)\n",
    "\n",
    "# two different cost functions \n",
    "# elbow\n",
    "elbow_score = []\n",
    "for k in range(2,20):\n",
    "    k_means = KMeans(init='k-means++', n_clusters=k, n_init=5)\n",
    "    k_means.fit(reduced_data)\n",
    "    elbow_score.append(k_means.inertia_)\n",
    "\n",
    "# sihouette\n",
    "sihouette_score = []\n",
    "for k in range(2,20):\n",
    "    k_means = KMeans(init='k-means++', n_clusters=k, n_init=5)\n",
    "    k_means.fit(reduced_data)\n",
    "    sihouette_score.append(metrics.silhouette_score(reduced_data, k_means.labels_))\n",
    "\n",
    "score = pd.DataFrame()\n",
    "score['elbow'] = elbow_score\n",
    "score['sihouette'] = sihouette_score\n",
    "\n",
    "score\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(5,8)\n",
    "plt.plot(range(2,20), score['elbow'], 'b*-')\n",
    "plt.xlim(1, plt.xlim()[1])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(5,8)\n",
    "plt.plot(range(2,20), score['sihouette'], 'b*-')\n",
    "plt.xlim(1, plt.xlim()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(8,5)\n",
    "plt.plot(range(2,20), score['elbow'], 'b*-')\n",
    "plt.xlim(1, plt.xlim()[1])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(8,5)\n",
    "plt.plot(range(2,20), score['sihouette'], 'b*-')\n",
    "plt.xlim(1, plt.xlim()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here\n",
    "from scipy.cluster.hierarchy import ward, dendrogram\n",
    "\n",
    "linkage_matrix = ward(reduced_data)\n",
    "\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "fcluster = fcluster(linkage_matrix, 3, criterion='maxclust')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "cluster_list = []\n",
    "for cluster in fcluster:\n",
    "    cluster_list.append((i,cluster))\n",
    "    i +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one = \"\"\n",
    "two = \"\"\n",
    "three = \"\"\n",
    "\n",
    "for item in cluster_list:\n",
    "    if item[-1] == 1:\n",
    "        one = one + document[item[0]]\n",
    "    if item[-1] == 2:\n",
    "        two = two + document[item[0]]\n",
    "    if item[-1] == 3:\n",
    "        three = three + document[item[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in [one,two,three]:\n",
    "    wordcloud = WordCloud().generate(cluster)\n",
    "    plt.figure(figsize = (8, 8), facecolor = None) \n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")   \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one pos tag\n",
    "from collections import Counter\n",
    "one_counts = Counter(one.split()).most_common(100)\n",
    "one_counts[50:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_nlp = nlp(one[:800000])\n",
    "one_noun = list()\n",
    "one_ad = list()\n",
    "one_verb = list()\n",
    "for i, sent in enumerate(one_nlp.sents):\n",
    "    for token in sent:\n",
    "        if token.pos_ == 'NOUN':\n",
    "            one_noun.append(str(token))\n",
    "        if token.pos_ == 'VERB':\n",
    "            one_verb.append(str(token))\n",
    "        if token.pos_ == 'ADJ':\n",
    "            one_ad.append(str(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counter is useful for string\n",
    "one_noun_counts = Counter(one_noun).most_common(50)\n",
    "one_noun_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_ad_counts = Counter(one_ad).most_common(50)\n",
    "one_ad_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_verb_counts = Counter(one_verb).most_common(50)\n",
    "one_verb_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two pos tag\n",
    "two_counts = Counter(two.split()).most_common(100)\n",
    "# two_counts[50:100]\n",
    "two_counts[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_nlp = nlp(two)\n",
    "two_noun = list()\n",
    "two_ad = list()\n",
    "two_verb = list()\n",
    "for i, sent in enumerate(two_nlp.sents):\n",
    "    for token in sent:\n",
    "        if token.pos_ == 'NOUN':\n",
    "            two_noun.append(str(token))\n",
    "        if token.pos_ == 'VERB':\n",
    "            two_verb.append(str(token))\n",
    "        if token.pos_ == 'ADJ':\n",
    "            two_ad.append(str(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_pos_count = pd.DataFrame()\n",
    "two_noun_counts = Counter(two_noun).most_common(50)\n",
    "two_pos_count['noun']=two_noun_counts\n",
    "two_ad_counts = Counter(two_ad).most_common(50)\n",
    "two_pos_count['adj'] = two_ad_counts\n",
    "two_verb_counts = Counter(two_verb).most_common(50)\n",
    "two_pos_count['verb'] = two_verb_counts\n",
    "two_pos_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# three pos tag\n",
    "three_counts = Counter(three.split()).most_common(100)\n",
    "# three_counts[50:100]\n",
    "three_counts[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_nlp = nlp(three)\n",
    "three_noun = list()\n",
    "three_ad = list()\n",
    "three_verb = list()\n",
    "for i, sent in enumerate(three_nlp.sents):\n",
    "    for token in sent:\n",
    "        if token.pos_ == 'NOUN':\n",
    "            three_noun.append(str(token))\n",
    "        if token.pos_ == 'VERB':\n",
    "            three_verb.append(str(token))\n",
    "        if token.pos_ == 'ADJ':\n",
    "            three_ad.append(str(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_pos_count = pd.DataFrame()\n",
    "three_noun_counts = Counter(three_noun).most_common(50)\n",
    "three_pos_count['noun']=three_noun_counts\n",
    "three_ad_counts = Counter(three_ad).most_common(50)\n",
    "three_pos_count['adj'] = three_ad_counts\n",
    "three_verb_counts = Counter(three_verb).most_common(50)\n",
    "three_pos_count['verb'] = three_verb_counts\n",
    "three_pos_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_noun_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_ad_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_verb_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one = \"\"\n",
    "two = \"\"\n",
    "three = \"\"\n",
    "\n",
    "for item in cluster_list:\n",
    "    if item[-1] == 1:\n",
    "        one = one + document[item[0]]\n",
    "    if item[-1] == 2:\n",
    "        two = two + document[item[0]]\n",
    "    if item[-1] == 3:\n",
    "        three = three + document[item[0]]\n",
    "def topic_tweets(cluster,topic,number):\n",
    "    one_nlp = nlp(cluster[:800000])\n",
    "    rain = \" \"\n",
    "    one_noun = list()\n",
    "    one_ad = list()\n",
    "    one_verb = list()\n",
    "    for i, sent in enumerate(one_nlp.sents):\n",
    "        for token in sent:\n",
    "            if str(token) == topic:\n",
    "                rain = rain + str(sent)\n",
    "    rain_nlp = nlp(rain)\n",
    "    rain_noun = list()\n",
    "    rain_ad = list()\n",
    "    rain_verb = list()\n",
    "    for i, sent in enumerate(rain_nlp.sents):\n",
    "        for token in sent:\n",
    "            if token.pos_ == 'NOUN':\n",
    "                rain_noun.append(str(token))\n",
    "            if token.pos_ == 'VERB':\n",
    "                rain_verb.append(str(token))\n",
    "            if token.pos_ == 'ADJ':\n",
    "                rain_ad.append(str(token))\n",
    "    rain_pos_count = pd.DataFrame()\n",
    "    rain_noun_counts = Counter(rain_noun).most_common(number)\n",
    "    rain_pos_count['noun']=rain_noun_counts\n",
    "    rain_ad_counts = Counter(rain_ad).most_common(number)\n",
    "    rain_pos_count['adj'] = rain_ad_counts\n",
    "    rain_verb_counts = Counter(rain_verb).most_common(number)\n",
    "    rain_pos_count['verb'] = rain_verb_counts\n",
    "    return rain_pos_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain = topic_tweets(one,'rain',20)\n",
    "rain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_two = topic_tweets(two,'rain',20)\n",
    "rain_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_three = topic_tweets(three,'rain',20)\n",
    "rain_three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekend = topic_tweets(one,'weekend',20)\n",
    "weekend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_weekend = topic_tweets(two,'weekend',10)\n",
    "two_weekend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_weekend = topic_tweets(three,'weekend',20)\n",
    "three_weekend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headache = topic_tweets(one,'headache',20)\n",
    "headache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headache = topic_tweets(two,'headache',10)\n",
    "headache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headache = topic_tweets(three,'headache',20)\n",
    "headache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work = topic_tweets(one,'work',20)\n",
    "work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work = topic_tweets(two,'work',20)\n",
    "work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work = topic_tweets(three,'work',20)\n",
    "work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "but = topic_tweets(one,'but',20)\n",
    "but"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "but = topic_tweets(two,'but',20)\n",
    "but"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "but = topic_tweets(three,'but',20)\n",
    "but"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "night = topic_tweets(one,'night',20)\n",
    "night"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "night = topic_tweets(two,'night',10)\n",
    "night"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "night = topic_tweets(three,'night',20)\n",
    "night"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Naive Bayes, Maximum Entropy, and SVM)\n",
    "# LogisticRegression\n",
    "# df = pd.read_csv('training.csv',encoding='latin-1',names=['polarity','id','date','query','user','text'])\n",
    "# df = df.iloc[100000:,:]\n",
    "# df.polarity.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add topic label\n",
    "def add_topics(document,df,topic):\n",
    "    topic_labels = []\n",
    "    for tweet in document:\n",
    "        label = 0\n",
    "        tweet=text_clean(tweet)\n",
    "        for token in tweet.split():\n",
    "            if token == topic:\n",
    "                label = 1\n",
    "        topic_labels.append(label)\n",
    "    \n",
    "    df[topic]=topic_labels\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = df['text']\n",
    "for i in [\"weather\",\"rain\",\"sun\",\"work\",\"job\",\"homework\",\"school\",\"kids\",\"friends\",\"he\",\"her\",\"she\",\"him\",\"sleep\",\"phone\"]:\n",
    "    df = add_topics(document,df,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tfidf']= dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,6:]\n",
    "y = df.iloc[:,0]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# always predicts the most frequent label in the training set.\n",
    "clf = DummyClassifier(strategy='most_frequent',random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "# Returns the mean accuracy on the given test data and labels.\n",
    "# Estimate the accuracy of the classifier on future data, using the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred,normalize = True)\n",
    "# Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "print (accuracy,recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import sklearn.ensemble as skens\n",
    "import sklearn.metrics as skmetric\n",
    "import sklearn.naive_bayes as sknb\n",
    "import sklearn.tree as sktree\n",
    "# fold = 10\n",
    "rf_model_10 = skens.RandomForestClassifier(n_estimators=10,oob_score=True, criterion='entropy')\n",
    "rf_model_10.fit(X_train,y_train)\n",
    "\n",
    "print(\"For test dataset: \", rf_model_10.score(X_test, y_test))\n",
    "feat_importance_10 = rf_model_10.feature_importances_\n",
    "# pd.DataFrame({'Feature Importance':feat_importance},\n",
    "#             index=df_mb_train.columns[:-1]).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validation by GridSearch\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "                 'n_estimators': [5, 10, 15, 20, 25, 30, 35, 40, 45, 50,55,60,65,70,75,80],\n",
    "                 'max_depth': range(1,30)\n",
    "             }\n",
    "rf_model = skens.RandomForestClassifier()\n",
    "grid_clf = GridSearchCV(rf_model, param_grid, cv=10)\n",
    "grid_clf.fit(X_train,y_train)\n",
    "print(grid_clf.best_estimator_)\n",
    "print(grid_clf.best_params_)\n",
    "print(grid_clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "gnb_model = sknb.GaussianNB()\n",
    "gnb_model.fit(X_train,y_train)\n",
    "\n",
    "print(\"For test dataset: \", gnb_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most important feature is URL_Length\n",
    "# gnb_model_mf = sknb.GaussianNB()\n",
    "# gnb_model_mf.fit(df_mb_train[['URL_LENGTH']],df_mb_train.Type)\n",
    "# print(\"For validation dataset: \", gnb_model_mf.score(df_mb_validation[['URL_LENGTH']], df_mb_validation.Type))\n",
    "# print(\"For test dataset: \", gnb_model_mf.score(df_mb_test_scaled[['URL_LENGTH']], df_mb_test_scaled.Type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def answer_six():    \n",
    "#     # YOUR CODE HERE\n",
    "#     clf = LogisticRegression()\n",
    "#     grid_values = {'C': [0.01, 0.1, 0.5, 1, 10, 100], 'penalty': ['l1', 'l2']}\n",
    "#     grid_search = GridSearchCV(clf, param_grid=grid_values, scoring='recall')\n",
    "#     grid_search.fit(X_train, y_train)\n",
    "#     cv_result = grid_search.cv_results_\n",
    "#     mean_test_score = cv_result['mean_test_score']\n",
    "#     result = np.array(mean_test_score).reshape(6,2)\n",
    "#     return result\n",
    "# answer_six()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def GridSearch_Heatmap(scores):\n",
    "#     %matplotlib inline\n",
    "#     import seaborn as sns\n",
    "#     import matplotlib.pyplot as plt\n",
    "#     plt.figure()\n",
    "#     sns.heatmap(scores.reshape(6,2), xticklabels=['l1','l2'], yticklabels=[0.01, 0.1, 0.5, 1, 10, 100])\n",
    "#     plt.yticks(rotation=0);\n",
    "# GridSearch_Heatmap(answer_six())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
